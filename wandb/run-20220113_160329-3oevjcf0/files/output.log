Loading data and splitting training and validation set...
Loading configuration...
Loading model...
/Users/weis/Documents/Skole/DTU/10_semester/ML_Ops/ML_Ops_ExamProject/src/data/make_dataset.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.encodings = {key: torch.tensor(val) for key, val in encodings.items()}
/Users/weis/Documents/Skole/DTU/10_semester/ML_Ops/ML_Ops_ExamProject/src/data/make_dataset.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels)
Setting up optimizer...
Setting up scheduler...
Training model...
Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|                                                                                                 | 0/8 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/Users/weis/Documents/Skole/DTU/10_semester/ML_Ops/ML_Ops_ExamProject/src/models/train_model.py", line 420, in <module>
    run()
  File "/Users/weis/Documents/Skole/DTU/10_semester/ML_Ops/ML_Ops_ExamProject/src/models/train_model.py", line 387, in run
    train_loss, train_acc = train(model,
  File "/Users/weis/Documents/Skole/DTU/10_semester/ML_Ops/ML_Ops_ExamProject/src/models/train_model.py", line 224, in train
    outputs = model(**batch)
  File "/opt/anaconda3/envs/Py39_MLOps/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/anaconda3/envs/Py39_MLOps/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1370, in forward
    transformer_outputs = self.transformer(
  File "/opt/anaconda3/envs/Py39_MLOps/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/anaconda3/envs/Py39_MLOps/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 887, in forward
    outputs = block(
  File "/opt/anaconda3/envs/Py39_MLOps/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/anaconda3/envs/Py39_MLOps/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 395, in forward
    attn_outputs = self.attn(
  File "/opt/anaconda3/envs/Py39_MLOps/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/anaconda3/envs/Py39_MLOps/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 340, in forward
    attn_output = self.resid_dropout(attn_output)
  File "/opt/anaconda3/envs/Py39_MLOps/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/anaconda3/envs/Py39_MLOps/lib/python3.9/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/opt/anaconda3/envs/Py39_MLOps/lib/python3.9/site-packages/torch/nn/functional.py", line 1169, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
KeyboardInterrupt
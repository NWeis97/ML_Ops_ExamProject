/Users/weis/Documents/Skole/DTU/10_semester/ML_Ops/ML_Ops_ExamProject/src/data/make_dataset.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.encodings = {key: torch.tensor(val) for key, val in encodings.items()}
/Users/weis/Documents/Skole/DTU/10_semester/ML_Ops/ML_Ops_ExamProject/src/data/make_dataset.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels)
Loading data and splitting training and validation set...
Loading configuration...
Loading model...
Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
  0%|                                                                                                 | 0/8 [00:00<?, ?it/s]
Setting up optimizer...
Setting up scheduler...


 25%|██████████████████████▎                                                                  | 2/8 [02:59<08:42, 87.12s/it]
Epoch: 1/4
Training_loss: 3.414924681186676
Training_accuracy: 0.5083333333333333
Validation_loss: 2.4562911987304688
 25%|██████████████████████▎                                                                  | 2/8 [02:59<08:42, 87.12s/it]Traceback (most recent call last):
  File "/Users/weis/Documents/Skole/DTU/10_semester/ML_Ops/ML_Ops_ExamProject/src/models/train_model.py", line 411, in <module>
    run()
  File "/Users/weis/Documents/Skole/DTU/10_semester/ML_Ops/ML_Ops_ExamProject/src/models/train_model.py", line 378, in run
    train_loss, train_acc = train(model,
  File "/Users/weis/Documents/Skole/DTU/10_semester/ML_Ops/ML_Ops_ExamProject/src/models/train_model.py", line 221, in train
    loss.backward()
  File "/opt/anaconda3/envs/Py39_MLOps/lib/python3.9/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/opt/anaconda3/envs/Py39_MLOps/lib/python3.9/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
  File "/opt/anaconda3/envs/Py39_MLOps/lib/python3.9/site-packages/wandb/wandb_torch.py", line 282, in <lambda>
    handle = var.register_hook(lambda grad: _callback(grad, log_track))
KeyboardInterrupt
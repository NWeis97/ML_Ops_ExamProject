#config.yaml
hyperparameters:
  batch_size: 128
  batch_ratio_validation: 0.2
  lr: 0.01
  lr_scheduler: 'cosine' #see https://huggingface.co/docs/transformers/main_classes/optimizer_schedules for possible types ['linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup']
  warmup_step_perc: 0 #Only makes sense if lr_scheduler has warmup
  epochs: 10
  seed: 1234
  optimizer_type: 'adamw'
  weight_decay: 0.02
